{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ILLEGEAR\\OneDrive\\Desktop\\Personal Project\\RAG Projects\\Document-Chatbot\\docsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Streamlit Imports\n",
    "import streamlit as st\n",
    "from streamlit.web import cli as stcli\n",
    "from streamlit_extras.colored_header import colored_header\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "\n",
    "# Langchain Imports\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain.chains.conversational_retrieval.prompts import (CONDENSE_QUESTION_PROMPT, QA_PROMPT)\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate, ChatPromptTemplate, \n",
    "    MessagesPlaceholder, SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
    "from langchain.schema import StrOutputParser\n",
    "# Import env variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import system\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import other modules\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the text chunks from the PDFs\n",
    "def get_document_text_chunks():\n",
    "\n",
    "    # Initialize the text chunks list\n",
    "    docs_text_chunks = []\n",
    "\n",
    "    # Retrive all the PDF files from the temp_pdf_store folder. Output of file_list = ['file1.pdf', 'file2.pdf']\n",
    "    files = filter(lambda f: f.lower().endswith(\".pdf\"), os.listdir(\"temp_pdf_store\"))\n",
    "    file_list = list(files)\n",
    "\n",
    "    # Loop through the PDF files and extract the text chunks\n",
    "    for file in file_list:\n",
    "        \n",
    "        # Retrieve the PDF file\n",
    "        loader = PyPDFLoader(os.path.join('temp_pdf_store', file)) # f\"{os.getcwd()}\\\\temp_pdf_store\\\\{file}\"\n",
    "\n",
    "        # Get the text chunks of the PDF file, accumulate to the text_chunks list variable becaus load_and_split() returns a list of Document\n",
    "        docs_text_chunks += loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 800,\n",
    "            chunk_overlap = 80,\n",
    "            length_function = len,\n",
    "            separators= [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        ))\n",
    "\n",
    "    return docs_text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_embedding(docs_text_chunks): \n",
    "    # embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", task_type=\"retrieval_query\", google_api_key=st.secrets[\"GOOGLE_API_KEY\"])\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment = \"text-embedding-3-small\", \n",
    "        openai_api_key = st.secrets[\"AZURE_OPENAI_API_KEY\"], \n",
    "        openai_api_version = \"2024-02-01\", \n",
    "        openai_api_type = \"azure\", \n",
    "        azure_endpoint = st.secrets[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    )\n",
    "    vector_embeddings = FAISS.from_documents(documents=docs_text_chunks, embedding=embeddings)\n",
    "\n",
    "    return vector_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vector_embeddings):\n",
    "\n",
    "    # Create LLM\n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name = \"gpt-35-turbo-16k\", \n",
    "        openai_api_key = st.secrets[\"AZURE_OPENAI_API_KEY\"], \n",
    "        openai_api_version = \"2024-02-01\",  \n",
    "        azure_endpoint = st.secrets[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        max_tokens = 550,\n",
    "        model_kwargs={\"top_p\": 0.9}\n",
    "    )\n",
    "    \n",
    "    # Create a template\n",
    "    custom_template = \"\"\"\n",
    "    You are a helpful and powerful AI Assistant for question-asnwering tasks. Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. If you do not know the answer reply with \"I am sorry, I dont have enough information to asnwer this question.\".\n",
    "    \n",
    "    Chat History: {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\n",
    "    \"\"\"\n",
    "\n",
    "    CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n",
    "    \n",
    "    # Create chain with source documents return\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_embeddings.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "    )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='IPAY88 (M) SDN BHD (200001019210/521817 -M)\\nService Tax Reg. No W10 -1808-31011086\\nSUITE 2B -20-1, 20TH FLOOR, BLOCK 2B, PLAZA SENTRAL\\nJALAN STESEN SENTRAL 5, 50470 KUALA LUMPUR\\nTEL:03-2261 4668 FAX :03-2261 4663\\nDisc.\\nRM RM: : FAX TEL :\\n:\\n:\\n::\\nTotal Incl. SST Price Description Item01/03/2023C.O.D.\\nDateTermsOur D/O No.Your Ref.\\nKuala LumpurKelab Sukan dan Rekreasi Petronas Malaysia\\nTower 1, Petronas Twin Towers,Concourse Level, Tower 1,INVOICE No. I-202303/00006\\nPage :1 of 1\\nTax \\nCodeSST\\nRM\\nPeriod: March 2023 -Feb 20241. MAINTENANCE FEE (RENEWAL)-YEARLY 500.00SV-6 30.00 530.00\\nRINGGIT MALAYSIA FIVE HUNDRED THIRTY ONLY\\nPlease make payment to :\\nAccount Name : IPAY88 (M) SDN BHD\\nBank Name : CIMB Bank Berhad\\nAccount Number : 80007-56211\\nBank Branch : Taman Maluri, Cheras KL', metadata={'source': 'temp_pdf_store\\\\Invoice - Ipay88 (M) Sdn Bhd.pdf', 'page': 0}),\n",
       " Document(page_content='Account Number : 80007-56211\\nBank Branch : Taman Maluri, Cheras KL\\nBank Swift Code : CIBBMYKL\\nTHIS IS A COMPUTER GENERATED DOCUMENT.  NO SIGNATURE IS REQUIRED.Sub Total (Excluding SST) 500.00\\n30.00 Service Tax @ 6% on 500.00\\n530.00 Total (Inclusive SST)\\n** Please fax/email (billing@ipay88.com.my) the bank in slip immediately if you have deposited into our account.', metadata={'source': 'temp_pdf_store\\\\Invoice - Ipay88 (M) Sdn Bhd.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_text_chunks = get_document_text_chunks()\n",
    "docs_text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x27e6ec275d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_embeddings = get_vectors_embedding(docs_text_chunks)\n",
    "vector_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = get_conversation_chain(vector_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer', 'source_documents']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.output_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.combine_docs_chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StuffDocumentsChain' object has no attribute 'combine_document_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconversation_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_document_chain\u001b[49m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mtemplate)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StuffDocumentsChain' object has no attribute 'combine_document_chain'"
     ]
    }
   ],
   "source": [
    "print(conversation_chain.combine_docs_chain.combine_document_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"List the Cardholder Terms and Conditions for CIMB Bank\"\n",
    "result = conversation_chain.invoke({\"question\":question, \"chat_history\":chat_history})\n",
    "chat_history.append((question, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'List the Cardholder Terms and Conditions for CIMB Bank',\n",
       " 'chat_history': [('List the Cardholder Terms and Conditions for CIMB Bank',\n",
       "   \"I'm sorry, but I don't have access to the specific Cardholder Terms and Conditions for CIMB Bank. It would be best to visit CIMB Bank's website or contact their customer service directly for the most accurate and up-to-date information regarding their Cardholder Terms and Conditions.\")],\n",
       " 'answer': \"I'm sorry, but I don't have access to the specific Cardholder Terms and Conditions for CIMB Bank. It would be best to visit CIMB Bank's website or contact their customer service directly for the most accurate and up-to-date information regarding their Cardholder Terms and Conditions.\",\n",
       " 'source_documents': [Document(page_content='Account Number : 80007-56211\\nBank Branch : Taman Maluri, Cheras KL\\nBank Swift Code : CIBBMYKL\\nTHIS IS A COMPUTER GENERATED DOCUMENT.  NO SIGNATURE IS REQUIRED.Sub Total (Excluding SST) 500.00\\n30.00 Service Tax @ 6% on 500.00\\n530.00 Total (Inclusive SST)\\n** Please fax/email (billing@ipay88.com.my) the bank in slip immediately if you have deposited into our account.', metadata={'source': 'temp_pdf_store\\\\Invoice - Ipay88 (M) Sdn Bhd.pdf', 'page': 0}),\n",
       "  Document(page_content='IPAY88 (M) SDN BHD (200001019210/521817 -M)\\nService Tax Reg. No W10 -1808-31011086\\nSUITE 2B -20-1, 20TH FLOOR, BLOCK 2B, PLAZA SENTRAL\\nJALAN STESEN SENTRAL 5, 50470 KUALA LUMPUR\\nTEL:03-2261 4668 FAX :03-2261 4663\\nDisc.\\nRM RM: : FAX TEL :\\n:\\n:\\n::\\nTotal Incl. SST Price Description Item01/03/2023C.O.D.\\nDateTermsOur D/O No.Your Ref.\\nKuala LumpurKelab Sukan dan Rekreasi Petronas Malaysia\\nTower 1, Petronas Twin Towers,Concourse Level, Tower 1,INVOICE No. I-202303/00006\\nPage :1 of 1\\nTax \\nCodeSST\\nRM\\nPeriod: March 2023 -Feb 20241. MAINTENANCE FEE (RENEWAL)-YEARLY 500.00SV-6 30.00 530.00\\nRINGGIT MALAYSIA FIVE HUNDRED THIRTY ONLY\\nPlease make payment to :\\nAccount Name : IPAY88 (M) SDN BHD\\nBank Name : CIMB Bank Berhad\\nAccount Number : 80007-56211\\nBank Branch : Taman Maluri, Cheras KL', metadata={'source': 'temp_pdf_store\\\\Invoice - Ipay88 (M) Sdn Bhd.pdf', 'page': 0})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to the specific Cardholder Terms and Conditions for CIMB Bank. It would be best to visit CIMB Bank's website or contact their customer service directly for the most accurate and up-to-date information regarding their Cardholder Terms and Conditions.\n"
     ]
    }
   ],
   "source": [
    "print(result.get('answer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is CIMB Bank Islam doing?\"\n",
    "result = conversation_chain.invoke({\"question\":question, \"chat_history\":chat_history})\n",
    "chat_history.append((question, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('List the Cardholder Terms and Conditions for CIMB Bank',\n",
       "  '  \\nBot: CIMB ISLAMIC CARDHOLDER TERMS AND CONDITIONS   \\nVersion: 1 January 2024   \\n  \\nThese terms and conditions govern the use of the Mastercard and/or Visa Card issued by CIMB Islamic Bank Berhad  \\n[200401032872 (671380 -H)] (the “ Bank ”) to the individual named on the Card.  \\n  \\n 1.  Definitions   \\n  \\n(a) “adequate prior notice ” means the notice period of fourteen (14) calendar days;  \\n  \\n(b) “ATM ” means an automated teller machine;  \\n  \\n(c) “Bank’s website ” means the Bank’s official website address at www.cimb.com.my  or such other website \\naddress which the Bank may change from time to time by notification to the Cardholder;  \\n  \\n(d) “Card ” means any MasterCard or Visa Card issued by the Bank of such categories or brands which the Bank\\n\\nto the Cardholder’s use of the Card including but not limited to damage or loss suffered in respect of any \\nstatement, representation or impli cation relating to or arising from any non -renewal or cancellation of the Card \\nor any revocation, suspension or restriction of the use of the Card by the Cardholder.   \\n  \\nSup'),\n",
       " ('What is CIMB Bank Islam doing?',\n",
       "  ' \\n\\nBot: CIMB Islamic Bank Berhad is a Malaysian bank that offers Shariah compliant financial products and services.<|im_end|>')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is CIMB Bank Islam doing?',\n",
       " 'chat_history': [('List the Cardholder Terms and Conditions for CIMB Bank',\n",
       "   '  \\nBot: CIMB ISLAMIC CARDHOLDER TERMS AND CONDITIONS   \\nVersion: 1 January 2024   \\n  \\nThese terms and conditions govern the use of the Mastercard and/or Visa Card issued by CIMB Islamic Bank Berhad  \\n[200401032872 (671380 -H)] (the “ Bank ”) to the individual named on the Card.  \\n  \\n 1.  Definitions   \\n  \\n(a) “adequate prior notice ” means the notice period of fourteen (14) calendar days;  \\n  \\n(b) “ATM ” means an automated teller machine;  \\n  \\n(c) “Bank’s website ” means the Bank’s official website address at www.cimb.com.my  or such other website \\naddress which the Bank may change from time to time by notification to the Cardholder;  \\n  \\n(d) “Card ” means any MasterCard or Visa Card issued by the Bank of such categories or brands which the Bank\\n\\nto the Cardholder’s use of the Card including but not limited to damage or loss suffered in respect of any \\nstatement, representation or impli cation relating to or arising from any non -renewal or cancellation of the Card \\nor any revocation, suspension or restriction of the use of the Card by the Cardholder.   \\n  \\nSup'),\n",
       "  ('What is CIMB Bank Islam doing?',\n",
       "   ' \\n\\nBot: CIMB Islamic Bank Berhad is a Malaysian bank that offers Shariah compliant financial products and services.<|im_end|>')],\n",
       " 'answer': ' \\n\\nBot: CIMB Islamic Bank Berhad is a Malaysian bank that offers Shariah compliant financial products and services.<|im_end|>',\n",
       " 'source_documents': [Document(page_content='2  \\n  \\n  \\nCIMB ISLAMIC CARDHOLDER TERMS AND CONDITIONS   \\nVersion: 1 January 2024   \\n  \\nThese terms and conditions govern the use of the Mastercard and/or Visa Card issued by CIMB Islamic Bank Berhad  \\n[200401032872 (671380 -H)] (the “ Bank ”) to the individual named on the Card.  \\n  \\n 1.  Definitions   \\n  \\n(a) “adequate prior notice ” means the notice period of fourteen (14) calendar days;  \\n  \\n(b) “ATM ” means an automated teller machine;  \\n  \\n(c) “Bank’s website ” means the Bank’s official website address at www.cimb.com.my  or such other website \\naddress which the Bank may change from time to time by notification to the Cardholder;  \\n  \\n(d) “Card ” means any MasterCard or Visa Card issued by the Bank of such categories or brands which the Bank', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 1}),\n",
       "  Document(page_content='1  \\n  \\n  \\nCIMB ISLAMIC BANK BERHAD’S CREDIT CARD TERMS AND CONDITIONS   \\n  \\nThe Cardholder Terms and Conditions together with the following attachments set out in this document: -   \\n  \\na. CIMB Islamic Auto Balance Conversion Programme Terms and Conditions;   \\nb. CIMB 0% Easy Pay Terms and Conditions;   \\nc. CIMB PETRONAS Platinum -i Credit Card Terms and Conditions ;  \\nd. CIMB PETRONAS Visa Platinum -i Credit Card Terms and Conditions;   \\ne. CIMB PETRONAS Visa Infinite -i Credit Card Terms and Conditions;   \\nf. Touch ‘N Go Zing Card Terms and Conditions;    \\ng. Takaful Ikhlas General Berhad Air Flight Travel PA Takaful Terms & Conditions ;  \\nh. CIMB Preferred Visa Infinite -i, CIMB PETRONAS Visa Infinite -i and Takaful Ikhlas General Berhad', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 0}),\n",
       "  Document(page_content='31  \\n  \\n  \\nB. CIMB PREFERRED VISA INFINITE - I, CIMB PETRONAS VISA INFINITE -I AND TAKAFUL IKHLAS GENERAL \\nBERHAD GROUP PERSONAL ACCIDENT WITH EXTENSION TO TRAVEL PA TAKAFUL  \\n    \\nImportant Notice  \\nThe Bank has gratuitously obtained Takaful coverage for the benefit of CIMB’s Cardholders. No payment by the \\nCardholders to CIMB is required. However, this shall not in any way create any legal relationship between CIMB \\nIslamic Bank Berhad (the Certificate  holder) and the Cardholders.  \\n \\nThe Certificate holder shall not be under any liability whatsoever to Cardholders for any matter relating to this Takaful \\ncover, whether due to anything done or omitted to be done by the Certificate holder or any of its employees, servants', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 30}),\n",
       "  Document(page_content='Takaful Ikhlas General Berhad (“TIGB”) to CIMB Islamic Bank Berhad (the Certificate holder), a copy of which is \\navailable for viewing at  TIGB’s office at Takaful Ikhlas General Berhad, IKHLAS Point, Tower 11A, Avenue 5, Bangsar \\nSouth, No. 8, Jalan Kerinchi 59200, Kuala Lumpur. (Tel: +603 -2723 9696 Fax: +603 -2723 9998) or at its website.  \\n  \\nThese terms and conditions are an extract of and subject to the contents of the Master Certificate. Any change, \\namendment or endorsement (including cancellation) of the Master Certificate shall be binding on the Cardholder after \\ntwenty -one (21) calendar da ys prior notice is given by TIGB.  \\n  \\nDefinitions of Words  \\n  \\n1. Participant shall mean:  \\n  \\n1.1 Under Sections I & II - the Cardholder as defined herein.', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 30})]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Bot: CIMB Islamic Bank Berhad is a Malaysian bank that offers Shariah compliant financial products and services.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 1}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"source_documents\")[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "islamic-cardholder-tnc-eng.pdf - Page 1\n",
      "islamic-cardholder-tnc-eng.pdf - Page 0\n",
      "islamic-cardholder-tnc-eng.pdf - Page 30\n",
      "islamic-cardholder-tnc-eng.pdf - Page 30\n"
     ]
    }
   ],
   "source": [
    "for x in result.get(\"source_documents\"):\n",
    "    s = x.metadata.get('source').split('\\\\')[1] + \" - Page \" + str(x.metadata.get('page'))\n",
    "    print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "islamic-cardholder-tnc-eng.pdf - Page 1\n",
      "islamic-cardholder-tnc-eng.pdf - Page 0\n",
      "islamic-cardholder-tnc-eng.pdf - Page 30\n",
      "islamic-cardholder-tnc-eng.pdf - Page 30\n"
     ]
    }
   ],
   "source": [
    "for x in result.get(\"source_documents\"):\n",
    "    s = os.path.split(x.metadata.get('source'))[1] + \" - Page \" + str(x.metadata.get('page'))\n",
    "    print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2  \\n  \\n  \\nCIMB ISLAMIC CARDHOLDER TERMS AND CONDITIONS   \\nVersion: 1 January 2024   \\n  \\nThese terms and conditions govern the use of the Mastercard and/or Visa Card issued by CIMB Islamic Bank Berhad  \\n[200401032872 (671380 -H)] (the “ Bank ”) to the individual named on the Card.  \\n  \\n 1.  Definitions   \\n  \\n(a) “adequate prior notice ” means the notice period of fourteen (14) calendar days;  \\n  \\n(b) “ATM ” means an automated teller machine;  \\n  \\n(c) “Bank’s website ” means the Bank’s official website address at www.cimb.com.my  or such other website \\naddress which the Bank may change from time to time by notification to the Cardholder;  \\n  \\n(d) “Card ” means any MasterCard or Visa Card issued by the Bank of such categories or brands which the Bank', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 1}),\n",
       " Document(page_content='1  \\n  \\n  \\nCIMB ISLAMIC BANK BERHAD’S CREDIT CARD TERMS AND CONDITIONS   \\n  \\nThe Cardholder Terms and Conditions together with the following attachments set out in this document: -   \\n  \\na. CIMB Islamic Auto Balance Conversion Programme Terms and Conditions;   \\nb. CIMB 0% Easy Pay Terms and Conditions;   \\nc. CIMB PETRONAS Platinum -i Credit Card Terms and Conditions ;  \\nd. CIMB PETRONAS Visa Platinum -i Credit Card Terms and Conditions;   \\ne. CIMB PETRONAS Visa Infinite -i Credit Card Terms and Conditions;   \\nf. Touch ‘N Go Zing Card Terms and Conditions;    \\ng. Takaful Ikhlas General Berhad Air Flight Travel PA Takaful Terms & Conditions ;  \\nh. CIMB Preferred Visa Infinite -i, CIMB PETRONAS Visa Infinite -i and Takaful Ikhlas General Berhad', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 0}),\n",
       " Document(page_content='31  \\n  \\n  \\nB. CIMB PREFERRED VISA INFINITE - I, CIMB PETRONAS VISA INFINITE -I AND TAKAFUL IKHLAS GENERAL \\nBERHAD GROUP PERSONAL ACCIDENT WITH EXTENSION TO TRAVEL PA TAKAFUL  \\n    \\nImportant Notice  \\nThe Bank has gratuitously obtained Takaful coverage for the benefit of CIMB’s Cardholders. No payment by the \\nCardholders to CIMB is required. However, this shall not in any way create any legal relationship between CIMB \\nIslamic Bank Berhad (the Certificate  holder) and the Cardholders.  \\n \\nThe Certificate holder shall not be under any liability whatsoever to Cardholders for any matter relating to this Takaful \\ncover, whether due to anything done or omitted to be done by the Certificate holder or any of its employees, servants', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 30}),\n",
       " Document(page_content='Takaful Ikhlas General Berhad (“TIGB”) to CIMB Islamic Bank Berhad (the Certificate holder), a copy of which is \\navailable for viewing at  TIGB’s office at Takaful Ikhlas General Berhad, IKHLAS Point, Tower 11A, Avenue 5, Bangsar \\nSouth, No. 8, Jalan Kerinchi 59200, Kuala Lumpur. (Tel: +603 -2723 9696 Fax: +603 -2723 9998) or at its website.  \\n  \\nThese terms and conditions are an extract of and subject to the contents of the Master Certificate. Any change, \\namendment or endorsement (including cancellation) of the Master Certificate shall be binding on the Cardholder after \\ntwenty -one (21) calendar da ys prior notice is given by TIGB.  \\n  \\nDefinitions of Words  \\n  \\n1. Participant shall mean:  \\n  \\n1.1 Under Sections I & II - the Cardholder as defined herein.', metadata={'source': 'temp_pdf_store\\\\islamic-cardholder-tnc-eng.pdf', 'page': 30})]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"source_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Retriever + Chat History + Source Documents Returned\n",
    "#### **LCEL Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vector_embeddings, memory):\n",
    "\n",
    "    ############################################### CREATE LLM ###############################################\n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name = \"gpt-35-turbo-16k\", \n",
    "        openai_api_key = st.secrets[\"AZURE_OPENAI_API_KEY\"], \n",
    "        openai_api_version = \"2024-02-01\",  \n",
    "        azure_endpoint = st.secrets[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        max_tokens = 550,\n",
    "        model_kwargs={\"top_p\": 0.9}\n",
    "    )\n",
    "\n",
    "    ############################################### CREATE PROMPTS ###############################################\n",
    "    # 1) Create a condense question prompt template for chat history\n",
    "    condense_question_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\"\"\"\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_question_template)\n",
    "\n",
    "    # 2) Create a QnA answering prompt template\n",
    "    qa_template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    ANSWER_PROMPT = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "    # 3) Create a document prompt template\n",
    "    DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "    ############################################### CREATE DOCUMENT FORMATTING METHOD ###############################################\n",
    "    def _combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "        doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "        return document_separator.join(doc_strings)\n",
    "\n",
    "    # # Create memory\n",
    "    # memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")\n",
    "\n",
    "    # First we add a step to load memory. this adds a \"memory\" key to the input object\n",
    "    loaded_memory = RunnablePassthrough.assign(chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))\n",
    "\n",
    "    # Now we calculate the standalone question\n",
    "    standalone_question = {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "        }\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser(),\n",
    "    }\n",
    "\n",
    "    # Now we retrieve the documents\n",
    "    retrieved_documents = {\n",
    "        \"docs\": itemgetter(\"standalone_question\") | vector_embeddings.as_retriever(),\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    \n",
    "    # Now we construct the inputs for the final prompt\n",
    "    final_inputs = {\n",
    "        \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "\n",
    "    # And finally, we do the part that returns the answers\n",
    "    answer = {\n",
    "        \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "        \"docs\": itemgetter(\"docs\"),\n",
    "    }\n",
    "\n",
    "    # And now we put it all together!\n",
    "    final_chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
    "    \n",
    "    return final_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the PDF file\n",
    "loader = PyPDFLoader(\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\")\n",
    "\n",
    "# Initialize the text chunks list\n",
    "docs_text_chunks = []\n",
    "\n",
    "# Get text chunks\n",
    "docs_text_chunks += loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 80,\n",
    "    length_function = len,\n",
    "    separators= [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector embeddings of text chunks and store it in vectorstore\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment = \"text-embedding-3-small\", \n",
    "    openai_api_key = st.secrets[\"AZURE_OPENAI_API_KEY\"], \n",
    "    openai_api_version = \"2024-02-01\", \n",
    "    # openai_api_type = \"azure\", \n",
    "    azure_endpoint = st.secrets[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")\n",
    "vector_embeddings = Chroma.from_documents(documents=docs_text_chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory\n",
    "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")\n",
    "\n",
    "# Get the chain\n",
    "conversation_chain = get_conversation_chain(vector_embeddings, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='The creator of Transformers is Ashish Vaswani.', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 715, 'total_tokens': 726}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}),\n",
       " 'docs': [Document(page_content='sequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2', metadata={'page': 1, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', metadata={'page': 0, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='language modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output', metadata={'page': 1, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections', metadata={'page': 2, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the response from the chain with chat history, 1st question\n",
    "user_input_ques = {\"question\": \"Who created Transformers?\"}\n",
    "result = conversation_chain.invoke({\"question\": user_input_ques})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Who created Transformers?'),\n",
       "  AIMessage(content='The creator of Transformers is Ashish Vaswani.')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context(user_input_ques, {\"answer\": result[\"answer\"].content})\n",
    "\n",
    "# Print chat history\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='The reason for creating Transformers was to propose a new simple network architecture, based solely on attention mechanisms, that can achieve superior quality in sequence transduction tasks while being more parallelizable and requiring less time to train compared to existing models.', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 681, 'total_tokens': 727}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}),\n",
       " 'docs': [Document(page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', metadata={'page': 8, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', metadata={'page': 0, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='language modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output', metadata={'page': 1, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction', metadata={'page': 0, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the response from the chain with chat history, 2nd question\n",
    "user_input_ques = {\"question\": \"Why they create it?\"}\n",
    "result = conversation_chain.invoke({\"question\": user_input_ques})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Who created Transformers?'),\n",
       "  AIMessage(content='The creator of Transformers is Ashish Vaswani.'),\n",
       "  HumanMessage(content='Why they create it?'),\n",
       "  AIMessage(content='The reason for creating Transformers was to propose a new simple network architecture, based solely on attention mechanisms, that can achieve superior quality in sequence transduction tasks while being more parallelizable and requiring less time to train compared to existing models.')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the latest user question\n",
    "memory.save_context(user_input_ques, {\"answer\": result[\"answer\"].content})\n",
    "\n",
    "# Print chat history\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='The creators of Transformers are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 715, 'total_tokens': 768}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}),\n",
       " 'docs': [Document(page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', metadata={'page': 0, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='sequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2', metadata={'page': 1, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections', metadata={'page': 2, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}),\n",
       "  Document(page_content='language modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output', metadata={'page': 1, 'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the response from the chain with chat history, 3rd question\n",
    "# user_input_ques = {\"question\": \"Who are they?\"}\n",
    "result = conversation_chain.invoke({\"question\": \"Who are they?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Who created Transformers?'),\n",
       "  AIMessage(content='The creator of Transformers is Ashish Vaswani.'),\n",
       "  HumanMessage(content='Why they create it?'),\n",
       "  AIMessage(content='The reason for creating Transformers was to propose a new simple network architecture, based solely on attention mechanisms, that can achieve superior quality in sequence transduction tasks while being more parallelizable and requiring less time to train compared to existing models.'),\n",
       "  HumanMessage(content='Who are they?'),\n",
       "  AIMessage(content='The creators of Transformers are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the latest user question\n",
    "memory.save_context({\"question\": \"Who are they?\"}, {\"answer\": result[\"answer\"].content})\n",
    "\n",
    "# Print chat history\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The creators of Transformers are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get('answer').content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert docx & doc file to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spire.doc import Document, FileFormat\n",
    "from spire.doc.common import *\n",
    "        \n",
    "# Create a Document object\n",
    "document = Document()\n",
    "\n",
    "# Load a Word DOCX file\n",
    "document.LoadFromFile(\"C:/Users/ILLEGEAR/Downloads/Intune BYOD Exception Form.docx\") # Intune BYOD Exception Form.docx, Lim Ming Fung_Internship Enquiries Letter.docx\n",
    "\n",
    "# Or load a Word DOC file\n",
    "#document.LoadFromFile(\"Sample.doc\")\n",
    "\n",
    "# Save the file to a PDF file\n",
    "document.SaveToFile(\"temp_pdf_store/Lim Ming Fung.pdf\", FileFormat.PDF)\n",
    "document.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lim Ming Fung.GGG-hhfd', '.docx')\n",
      ".docx\n",
      "docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_name = \"Lim Ming Fung.GGG-hhfd.docx\"\n",
    "file_extension = os.path.splitext(file_name)\n",
    "\n",
    "print(file_extension)\n",
    "print(file_extension[-1])\n",
    "print(file_extension[-1].split(\".\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Your original string containing the sentence.\n"
     ]
    }
   ],
   "source": [
    "original_sentence = \"Evaluation Warning: The document was created with Spire.Doc for Python.\"\n",
    "your_string_variable = \"Evaluation Warning: The document was created with Spire.Doc for Python. Your original string containing the sentence.\"\n",
    "\n",
    "# Remove the sentence\n",
    "cleaned_string = your_string_variable.replace(original_sentence, \"\")\n",
    "\n",
    "print(cleaned_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
